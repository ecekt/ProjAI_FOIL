{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Progbar\n",
    "from random import randint\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import inflect\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/ece/Desktop/nlp/vqa/FOIL-resources/coco/PythonAPI')\n",
    "sys.path.append('/home/ece/Desktop/nlp/vqa/FOIL-resources/coco/PythonAPI/pycocotools/')\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import nltk\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "from mosestokenizer import *\n",
    "tokenize = MosesTokenizer('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_foil_train = pd.read_csv(\n",
    "    \"foil_train.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"label\", \"caption\", \"jpg\"]\n",
    ")\n",
    "\n",
    "foil_words = set()\n",
    "with open(\"foilv1.0_train2017.json\") as in_file:\n",
    "    foil_train = json.load(in_file)\n",
    "    for annotation in foil_train[\"annotations\"]:\n",
    "        foil_words.add(annotation[\"foil_word\"])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'apple', 'backpack', 'ball', 'banana', 'bat', 'bear', 'bed', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'bowl', 'broccoli', 'bus', 'cake', 'car', 'carrot', 'cat', 'chair', 'clock', 'couch', 'cow', 'cup', 'dog', 'donut', 'drier', 'elephant', 'fork', 'frisbee', 'giraffe', 'glass', 'glove', 'handbag', 'horse', 'hydrant', 'keyboard', 'kite', 'knife', 'laptop', 'microwave', 'motorcycle', 'mouse', 'orange', 'oven', 'phone', 'pizza', 'racket', 'refrigerator', 'remote', 'sandwich', 'scissors', 'sheep', 'sink', 'skateboard', 'skis', 'snowboard', 'spoon', 'suitcase', 'surfboard', 'table', 'tie', 'toaster', 'toilet', 'toothbrush', 'train', 'truck', 'tv', 'umbrella', 'vase', 'zebra']\n"
     ]
    }
   ],
   "source": [
    "with open(\"cocoObject_target.txt\") as in_file:\n",
    "    obj_targ = [l.split()[0] for l in in_file.readlines()]\n",
    "    \n",
    "print(obj_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 203564, 'id': 37, 'caption': 'A bicycle replica with a clock as the front wheel.'}\n",
      "118287\n"
     ]
    }
   ],
   "source": [
    "with open(\"annotations_trainval/captions_train2017.json\") as in_file:\n",
    "    train_coco = json.load(in_file)\n",
    "\n",
    "train_captions_per_image = dict()\n",
    "\n",
    "print(train_coco['annotations'][0])\n",
    "\n",
    "for tc in train_coco['annotations']:\n",
    "    if tc['image_id'] in train_captions_per_image:\n",
    "        train_captions_per_image[tc['image_id']].append((tc['caption'], tc['id']))\n",
    "    else:\n",
    "        train_captions_per_image[tc['image_id']] = [(tc['caption'], tc['id'])]\n",
    "        \n",
    "print(len(train_captions_per_image)) #118287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_punctuation(captions_per_image_set):\n",
    "    \n",
    "    separated_set = dict()\n",
    "\n",
    "    count = 0\n",
    "    for tp in captions_per_image_set:\n",
    "    \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "        if tp in separated_set:\n",
    "            separated_set[int(tp)].append(get_tokenized_captions(captions_per_image_set[tp]))\n",
    "        else:\n",
    "            separated_set[int(tp)] = (get_tokenized_captions(captions_per_image_set[tp]))\n",
    "        \n",
    "    return separated_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"annotations_trainval/captions_val2017.json\") as in_file:\n",
    "#     val_coco = json.load(in_file)\n",
    "        \n",
    "# val_captions_per_image = dict()\n",
    "\n",
    "# for vc in val_coco['annotations']:\n",
    "#     if vc['image_id'] in val_captions_per_image:\n",
    "#         val_captions_per_image[vc['image_id']].append((vc['caption'], vc['id']))\n",
    "#     else:\n",
    "#         val_captions_per_image[vc['image_id']] = [(vc['caption'], vc['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#done\n",
    "# print('train')\n",
    "# train_captions_per_image = separate_punctuation(train_captions_per_image)\n",
    "# print('val')\n",
    "# val_captions_per_image = separate_punctuation(val_captions_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"train_dataset_sep_punc.json\") as in_file:\n",
    "    train_captions_per_image = json.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"val_dataset_sep_punc.json\") as in_file:\n",
    "    val_captions_per_image = json.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"annotations_test/image_info_test2017.json\") as in_file:\n",
    "    test_cap = json.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val_captions_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coco_url': 'http://images.cocodataset.org/test2017/000000466319.jpg',\n",
       " 'date_captured': '2013-11-14 11:04:33',\n",
       " 'file_name': '000000466319.jpg',\n",
       " 'height': 480,\n",
       " 'id': 466319,\n",
       " 'license': 6,\n",
       " 'width': 640}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cap['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1, 'name': 'person', 'supercategory': 'person'},\n",
       " {'id': 2, 'name': 'bicycle', 'supercategory': 'vehicle'},\n",
       " {'id': 3, 'name': 'car', 'supercategory': 'vehicle'},\n",
       " {'id': 4, 'name': 'motorcycle', 'supercategory': 'vehicle'},\n",
       " {'id': 5, 'name': 'airplane', 'supercategory': 'vehicle'},\n",
       " {'id': 6, 'name': 'bus', 'supercategory': 'vehicle'},\n",
       " {'id': 7, 'name': 'train', 'supercategory': 'vehicle'},\n",
       " {'id': 8, 'name': 'truck', 'supercategory': 'vehicle'},\n",
       " {'id': 9, 'name': 'boat', 'supercategory': 'vehicle'},\n",
       " {'id': 10, 'name': 'traffic light', 'supercategory': 'outdoor'},\n",
       " {'id': 11, 'name': 'fire hydrant', 'supercategory': 'outdoor'},\n",
       " {'id': 13, 'name': 'stop sign', 'supercategory': 'outdoor'},\n",
       " {'id': 14, 'name': 'parking meter', 'supercategory': 'outdoor'},\n",
       " {'id': 15, 'name': 'bench', 'supercategory': 'outdoor'},\n",
       " {'id': 16, 'name': 'bird', 'supercategory': 'animal'},\n",
       " {'id': 17, 'name': 'cat', 'supercategory': 'animal'},\n",
       " {'id': 18, 'name': 'dog', 'supercategory': 'animal'},\n",
       " {'id': 19, 'name': 'horse', 'supercategory': 'animal'},\n",
       " {'id': 20, 'name': 'sheep', 'supercategory': 'animal'},\n",
       " {'id': 21, 'name': 'cow', 'supercategory': 'animal'},\n",
       " {'id': 22, 'name': 'elephant', 'supercategory': 'animal'},\n",
       " {'id': 23, 'name': 'bear', 'supercategory': 'animal'},\n",
       " {'id': 24, 'name': 'zebra', 'supercategory': 'animal'},\n",
       " {'id': 25, 'name': 'giraffe', 'supercategory': 'animal'},\n",
       " {'id': 27, 'name': 'backpack', 'supercategory': 'accessory'},\n",
       " {'id': 28, 'name': 'umbrella', 'supercategory': 'accessory'},\n",
       " {'id': 31, 'name': 'handbag', 'supercategory': 'accessory'},\n",
       " {'id': 32, 'name': 'tie', 'supercategory': 'accessory'},\n",
       " {'id': 33, 'name': 'suitcase', 'supercategory': 'accessory'},\n",
       " {'id': 34, 'name': 'frisbee', 'supercategory': 'sports'},\n",
       " {'id': 35, 'name': 'skis', 'supercategory': 'sports'},\n",
       " {'id': 36, 'name': 'snowboard', 'supercategory': 'sports'},\n",
       " {'id': 37, 'name': 'sports ball', 'supercategory': 'sports'},\n",
       " {'id': 38, 'name': 'kite', 'supercategory': 'sports'},\n",
       " {'id': 39, 'name': 'baseball bat', 'supercategory': 'sports'},\n",
       " {'id': 40, 'name': 'baseball glove', 'supercategory': 'sports'},\n",
       " {'id': 41, 'name': 'skateboard', 'supercategory': 'sports'},\n",
       " {'id': 42, 'name': 'surfboard', 'supercategory': 'sports'},\n",
       " {'id': 43, 'name': 'tennis racket', 'supercategory': 'sports'},\n",
       " {'id': 44, 'name': 'bottle', 'supercategory': 'kitchen'},\n",
       " {'id': 46, 'name': 'wine glass', 'supercategory': 'kitchen'},\n",
       " {'id': 47, 'name': 'cup', 'supercategory': 'kitchen'},\n",
       " {'id': 48, 'name': 'fork', 'supercategory': 'kitchen'},\n",
       " {'id': 49, 'name': 'knife', 'supercategory': 'kitchen'},\n",
       " {'id': 50, 'name': 'spoon', 'supercategory': 'kitchen'},\n",
       " {'id': 51, 'name': 'bowl', 'supercategory': 'kitchen'},\n",
       " {'id': 52, 'name': 'banana', 'supercategory': 'food'},\n",
       " {'id': 53, 'name': 'apple', 'supercategory': 'food'},\n",
       " {'id': 54, 'name': 'sandwich', 'supercategory': 'food'},\n",
       " {'id': 55, 'name': 'orange', 'supercategory': 'food'},\n",
       " {'id': 56, 'name': 'broccoli', 'supercategory': 'food'},\n",
       " {'id': 57, 'name': 'carrot', 'supercategory': 'food'},\n",
       " {'id': 58, 'name': 'hot dog', 'supercategory': 'food'},\n",
       " {'id': 59, 'name': 'pizza', 'supercategory': 'food'},\n",
       " {'id': 60, 'name': 'donut', 'supercategory': 'food'},\n",
       " {'id': 61, 'name': 'cake', 'supercategory': 'food'},\n",
       " {'id': 62, 'name': 'chair', 'supercategory': 'furniture'},\n",
       " {'id': 63, 'name': 'couch', 'supercategory': 'furniture'},\n",
       " {'id': 64, 'name': 'potted plant', 'supercategory': 'furniture'},\n",
       " {'id': 65, 'name': 'bed', 'supercategory': 'furniture'},\n",
       " {'id': 67, 'name': 'dining table', 'supercategory': 'furniture'},\n",
       " {'id': 70, 'name': 'toilet', 'supercategory': 'furniture'},\n",
       " {'id': 72, 'name': 'tv', 'supercategory': 'electronic'},\n",
       " {'id': 73, 'name': 'laptop', 'supercategory': 'electronic'},\n",
       " {'id': 74, 'name': 'mouse', 'supercategory': 'electronic'},\n",
       " {'id': 75, 'name': 'remote', 'supercategory': 'electronic'},\n",
       " {'id': 76, 'name': 'keyboard', 'supercategory': 'electronic'},\n",
       " {'id': 77, 'name': 'cell phone', 'supercategory': 'electronic'},\n",
       " {'id': 78, 'name': 'microwave', 'supercategory': 'appliance'},\n",
       " {'id': 79, 'name': 'oven', 'supercategory': 'appliance'},\n",
       " {'id': 80, 'name': 'toaster', 'supercategory': 'appliance'},\n",
       " {'id': 81, 'name': 'sink', 'supercategory': 'appliance'},\n",
       " {'id': 82, 'name': 'refrigerator', 'supercategory': 'appliance'},\n",
       " {'id': 84, 'name': 'book', 'supercategory': 'indoor'},\n",
       " {'id': 85, 'name': 'clock', 'supercategory': 'indoor'},\n",
       " {'id': 86, 'name': 'vase', 'supercategory': 'indoor'},\n",
       " {'id': 87, 'name': 'scissors', 'supercategory': 'indoor'},\n",
       " {'id': 88, 'name': 'teddy bear', 'supercategory': 'indoor'},\n",
       " {'id': 89, 'name': 'hair drier', 'supercategory': 'indoor'},\n",
       " {'id': 90, 'name': 'toothbrush', 'supercategory': 'indoor'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cap['categories'] #object categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "person person\n",
      "bicycle bicycle\n",
      "car car\n",
      "motorcycle motorcycle\n",
      "airplane airplane\n",
      "bus bus\n",
      "train train\n",
      "truck truck\n",
      "boat boat\n",
      "traffic light traffic light\n",
      "fire hydrant fire hydrant\n",
      "stop sign stop sign\n",
      "parking meter parking meter\n",
      "bench bench\n",
      "bird bird\n",
      "cat cat\n",
      "dog dog\n",
      "horse horse\n",
      "sheep sheep\n",
      "cow cow\n",
      "elephant elephant\n",
      "bear bear\n",
      "zebra zebra\n",
      "giraffe giraffe\n",
      "backpack backpack\n",
      "umbrella umbrella\n",
      "handbag handbag\n",
      "tie tie\n",
      "suitcase suitcase\n",
      "frisbee frisbee\n",
      "skis skis\n",
      "snowboard snowboard\n",
      "sports ball sports ball\n",
      "kite kite\n",
      "baseball bat baseball bat\n",
      "baseball glove baseball glove\n",
      "skateboard skateboard\n",
      "surfboard surfboard\n",
      "tennis racket tennis racket\n",
      "bottle bottle\n",
      "wine glass wine glass\n",
      "cup cup\n",
      "fork fork\n",
      "knife knife\n",
      "spoon spoon\n",
      "bowl bowl\n",
      "banana banana\n",
      "apple apple\n",
      "sandwich sandwich\n",
      "orange orange\n",
      "broccoli broccoli\n",
      "carrot carrot\n",
      "hot dog hot dog\n",
      "pizza pizza\n",
      "donut donut\n",
      "cake cake\n",
      "chair chair\n",
      "couch couch\n",
      "potted plant potted plant\n",
      "bed bed\n",
      "dining table dining table\n",
      "toilet toilet\n",
      "tv tv\n",
      "laptop laptop\n",
      "mouse mouse\n",
      "remote remote\n",
      "keyboard keyboard\n",
      "cell phone cell phone\n",
      "microwave microwave\n",
      "oven oven\n",
      "toaster toaster\n",
      "sink sink\n",
      "refrigerator refrigerator\n",
      "book book\n",
      "clock clock\n",
      "vase vase\n",
      "scissors scissors\n",
      "teddy bear teddy bear\n",
      "hair drier hair drier\n",
      "toothbrush toothbrush\n"
     ]
    }
   ],
   "source": [
    "with open(\"annotations_test/image_info_test2014.json\") as in_file:\n",
    "    test_cap2014 = json.load(in_file)\n",
    "    \n",
    "print(len(test_cap2014['categories']))\n",
    "\n",
    "for c in range(len(test_cap2014['categories'])):\n",
    "    print(test_cap['categories'][c]['name'], test_cap2014['categories'][c]['name'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj_cat = []\n",
    "super_cat = []\n",
    "obj_sup_cat = dict()\n",
    "mult_obj_cat = []\n",
    "mult_sup_cat = dict()\n",
    "\n",
    "for c in test_cap['categories']:\n",
    "    if len(c['name'].split()) == 1:\n",
    "        obj_cat.append(c['name']) \n",
    "        super_cat.append(c['supercategory'])\n",
    "        \n",
    "        if c['supercategory'] in obj_sup_cat:\n",
    "            obj_sup_cat[c['supercategory']].append(c['name'])\n",
    "        else:\n",
    "            obj_sup_cat[c['supercategory']] = [c['name']]\n",
    "            \n",
    "    else:\n",
    "        mult_obj_cat.append(c['name'])\n",
    "        \n",
    "        if c['supercategory'] in mult_sup_cat:\n",
    "            mult_sup_cat[c['supercategory']].append(c['name'])\n",
    "        else:\n",
    "            mult_sup_cat[c['supercategory']] = [c['name']]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obj_cat = sorted(obj_cat)\n",
    "# #print(len(obj_cat)\n",
    "# print(obj_cat)\n",
    "# print()\n",
    "# print(len(mult_obj_cat), mult_obj_cat)\n",
    "\n",
    "all_cat = []\n",
    "all_cat.extend(obj_cat)\n",
    "all_cat.extend(mult_obj_cat)\n",
    "all_cat = sorted(all_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 ['airplane', 'apple', 'backpack', 'ball', 'banana', 'bat', 'bear', 'bed', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'bowl', 'broccoli', 'bus', 'cake', 'car', 'carrot', 'cat', 'chair', 'clock', 'couch', 'cow', 'cup', 'dog', 'donut', 'drier', 'elephant', 'fork', 'frisbee', 'giraffe', 'glass', 'glove', 'handbag', 'horse', 'hydrant', 'keyboard', 'kite', 'knife', 'laptop', 'microwave', 'motorcycle', 'mouse', 'orange', 'oven', 'phone', 'pizza', 'racket', 'refrigerator', 'remote', 'sandwich', 'scissors', 'sheep', 'sink', 'skateboard', 'skis', 'snowboard', 'spoon', 'suitcase', 'surfboard', 'table', 'tie', 'toaster', 'toilet', 'toothbrush', 'train', 'truck', 'tv', 'umbrella', 'vase', 'zebra']\n"
     ]
    }
   ],
   "source": [
    "#different compared to the file I received from Ravi \n",
    "#categories such as: dining table vs. table, sports ball vs. ball\n",
    "\n",
    "print(len(obj_targ), obj_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mismatches\n",
    "\n",
    "# R ball\n",
    "# R bat\n",
    "# R drier\n",
    "# R glass\n",
    "# R glove\n",
    "# R hydrant\n",
    "# R phone\n",
    "# R racket\n",
    "# R table\n",
    "\n",
    "\n",
    "# E 'baseball bat' bat\n",
    "# E 'baseball glove' glove\n",
    "# E 'cell phone' phone\n",
    "# E 'dining table' table\n",
    "# E 'fire hydrant' hydrant\n",
    "# E 'hair drier' drier\n",
    "# E 'hot dog' dog\n",
    "# E 'parking meter' NOT IN OLD LIST\n",
    "# E 'person' NOT IN OLD LIST  +person\n",
    "# E 'potted plant' NOT IN OLD LIST +plant\n",
    "# E 'sports ball' ball\n",
    "# E 'stop sign' NOT IN OLD LIST +sign\n",
    "# E 'teddy bear' NOT IN OLD LIST\n",
    "# E 'tennis racket'racket\n",
    "# E 'traffic light' NOT IN OLD LIST +light\n",
    "# E 'wine glass' glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obj_targ.extend(['person', 'plant', 'sign', 'light'])\n",
    "# obj_targ = sorted(obj_targ)\n",
    "\n",
    "with open('new_obj_categories.txt', 'w') as f:\n",
    "    \n",
    "    for c in range(len(obj_targ)):\n",
    "        f.write(obj_targ[c])    \n",
    "        if c < (len(obj_targ) - 1):\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('new_obj_categories.txt', 'r') as f:\n",
    "    foil2_categories = [l.split()[0] for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(foil2_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manual addition of single-word categories, obtained from multi-word counterparts\n",
    "\n",
    "obj_sup_cat['outdoor'].append('light')\n",
    "obj_sup_cat['outdoor'].append('hydrant')\n",
    "obj_sup_cat['outdoor'].append('sign')\n",
    "\n",
    "obj_sup_cat['sports'].append('ball')\n",
    "obj_sup_cat['sports'].append('bat')\n",
    "obj_sup_cat['sports'].append('glove')\n",
    "obj_sup_cat['sports'].append('racket')\n",
    "\n",
    "obj_sup_cat['kitchen'].append('glass')\n",
    "\n",
    "obj_sup_cat['furniture'].append('plant')\n",
    "obj_sup_cat['furniture'].append('table')\n",
    "\n",
    "obj_sup_cat['electronic'].append('phone')\n",
    "\n",
    "obj_sup_cat['indoor'].append('drier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for c in obj_sup_cat:\n",
    "    count += len(obj_sup_cat[c])\n",
    "    \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bicycle', 'car'), ('bicycle', 'motorcycle'), ('bicycle', 'airplane'), ('bicycle', 'bus'), ('bicycle', 'train'), ('bicycle', 'truck'), ('bicycle', 'boat'), ('car', 'bicycle'), ('car', 'motorcycle'), ('car', 'airplane'), ('car', 'bus'), ('car', 'train'), ('car', 'truck'), ('car', 'boat'), ('motorcycle', 'bicycle'), ('motorcycle', 'car'), ('motorcycle', 'airplane'), ('motorcycle', 'bus'), ('motorcycle', 'train'), ('motorcycle', 'truck'), ('motorcycle', 'boat'), ('airplane', 'bicycle'), ('airplane', 'car'), ('airplane', 'motorcycle'), ('airplane', 'bus'), ('airplane', 'train'), ('airplane', 'truck'), ('airplane', 'boat'), ('bus', 'bicycle'), ('bus', 'car'), ('bus', 'motorcycle'), ('bus', 'airplane'), ('bus', 'train'), ('bus', 'truck'), ('bus', 'boat'), ('train', 'bicycle'), ('train', 'car'), ('train', 'motorcycle'), ('train', 'airplane'), ('train', 'bus'), ('train', 'truck'), ('train', 'boat'), ('truck', 'bicycle'), ('truck', 'car'), ('truck', 'motorcycle'), ('truck', 'airplane'), ('truck', 'bus'), ('truck', 'train'), ('truck', 'boat'), ('boat', 'bicycle'), ('boat', 'car'), ('boat', 'motorcycle'), ('boat', 'airplane'), ('boat', 'bus'), ('boat', 'train'), ('boat', 'truck'), ('bench', 'light'), ('bench', 'hydrant'), ('bench', 'sign'), ('light', 'bench'), ('light', 'hydrant'), ('light', 'sign'), ('hydrant', 'bench'), ('hydrant', 'light'), ('hydrant', 'sign'), ('sign', 'bench'), ('sign', 'light'), ('sign', 'hydrant'), ('bird', 'cat'), ('bird', 'dog'), ('bird', 'horse'), ('bird', 'sheep'), ('bird', 'cow'), ('bird', 'elephant'), ('bird', 'bear'), ('bird', 'zebra'), ('bird', 'giraffe'), ('cat', 'bird'), ('cat', 'dog'), ('cat', 'horse'), ('cat', 'sheep'), ('cat', 'cow'), ('cat', 'elephant'), ('cat', 'bear'), ('cat', 'zebra'), ('cat', 'giraffe'), ('dog', 'bird'), ('dog', 'cat'), ('dog', 'horse'), ('dog', 'sheep'), ('dog', 'cow'), ('dog', 'elephant'), ('dog', 'bear'), ('dog', 'zebra'), ('dog', 'giraffe'), ('horse', 'bird'), ('horse', 'cat'), ('horse', 'dog'), ('horse', 'sheep'), ('horse', 'cow'), ('horse', 'elephant'), ('horse', 'bear'), ('horse', 'zebra'), ('horse', 'giraffe'), ('sheep', 'bird'), ('sheep', 'cat'), ('sheep', 'dog'), ('sheep', 'horse'), ('sheep', 'cow'), ('sheep', 'elephant'), ('sheep', 'bear'), ('sheep', 'zebra'), ('sheep', 'giraffe'), ('cow', 'bird'), ('cow', 'cat'), ('cow', 'dog'), ('cow', 'horse'), ('cow', 'sheep'), ('cow', 'elephant'), ('cow', 'bear'), ('cow', 'zebra'), ('cow', 'giraffe'), ('elephant', 'bird'), ('elephant', 'cat'), ('elephant', 'dog'), ('elephant', 'horse'), ('elephant', 'sheep'), ('elephant', 'cow'), ('elephant', 'bear'), ('elephant', 'zebra'), ('elephant', 'giraffe'), ('bear', 'bird'), ('bear', 'cat'), ('bear', 'dog'), ('bear', 'horse'), ('bear', 'sheep'), ('bear', 'cow'), ('bear', 'elephant'), ('bear', 'zebra'), ('bear', 'giraffe'), ('zebra', 'bird'), ('zebra', 'cat'), ('zebra', 'dog'), ('zebra', 'horse'), ('zebra', 'sheep'), ('zebra', 'cow'), ('zebra', 'elephant'), ('zebra', 'bear'), ('zebra', 'giraffe'), ('giraffe', 'bird'), ('giraffe', 'cat'), ('giraffe', 'dog'), ('giraffe', 'horse'), ('giraffe', 'sheep'), ('giraffe', 'cow'), ('giraffe', 'elephant'), ('giraffe', 'bear'), ('giraffe', 'zebra'), ('backpack', 'umbrella'), ('backpack', 'handbag'), ('backpack', 'tie'), ('backpack', 'suitcase'), ('umbrella', 'backpack'), ('umbrella', 'handbag'), ('umbrella', 'tie'), ('umbrella', 'suitcase'), ('handbag', 'backpack'), ('handbag', 'umbrella'), ('handbag', 'tie'), ('handbag', 'suitcase'), ('tie', 'backpack'), ('tie', 'umbrella'), ('tie', 'handbag'), ('tie', 'suitcase'), ('suitcase', 'backpack'), ('suitcase', 'umbrella'), ('suitcase', 'handbag'), ('suitcase', 'tie'), ('frisbee', 'skis'), ('frisbee', 'snowboard'), ('frisbee', 'kite'), ('frisbee', 'skateboard'), ('frisbee', 'surfboard'), ('frisbee', 'ball'), ('frisbee', 'bat'), ('frisbee', 'glove'), ('frisbee', 'racket'), ('skis', 'frisbee'), ('skis', 'snowboard'), ('skis', 'kite'), ('skis', 'skateboard'), ('skis', 'surfboard'), ('skis', 'ball'), ('skis', 'bat'), ('skis', 'glove'), ('skis', 'racket'), ('snowboard', 'frisbee'), ('snowboard', 'skis'), ('snowboard', 'kite'), ('snowboard', 'skateboard'), ('snowboard', 'surfboard'), ('snowboard', 'ball'), ('snowboard', 'bat'), ('snowboard', 'glove'), ('snowboard', 'racket'), ('kite', 'frisbee'), ('kite', 'skis'), ('kite', 'snowboard'), ('kite', 'skateboard'), ('kite', 'surfboard'), ('kite', 'ball'), ('kite', 'bat'), ('kite', 'glove'), ('kite', 'racket'), ('skateboard', 'frisbee'), ('skateboard', 'skis'), ('skateboard', 'snowboard'), ('skateboard', 'kite'), ('skateboard', 'surfboard'), ('skateboard', 'ball'), ('skateboard', 'bat'), ('skateboard', 'glove'), ('skateboard', 'racket'), ('surfboard', 'frisbee'), ('surfboard', 'skis'), ('surfboard', 'snowboard'), ('surfboard', 'kite'), ('surfboard', 'skateboard'), ('surfboard', 'ball'), ('surfboard', 'bat'), ('surfboard', 'glove'), ('surfboard', 'racket'), ('ball', 'frisbee'), ('ball', 'skis'), ('ball', 'snowboard'), ('ball', 'kite'), ('ball', 'skateboard'), ('ball', 'surfboard'), ('ball', 'bat'), ('ball', 'glove'), ('ball', 'racket'), ('bat', 'frisbee'), ('bat', 'skis'), ('bat', 'snowboard'), ('bat', 'kite'), ('bat', 'skateboard'), ('bat', 'surfboard'), ('bat', 'ball'), ('bat', 'glove'), ('bat', 'racket'), ('glove', 'frisbee'), ('glove', 'skis'), ('glove', 'snowboard'), ('glove', 'kite'), ('glove', 'skateboard'), ('glove', 'surfboard'), ('glove', 'ball'), ('glove', 'bat'), ('glove', 'racket'), ('racket', 'frisbee'), ('racket', 'skis'), ('racket', 'snowboard'), ('racket', 'kite'), ('racket', 'skateboard'), ('racket', 'surfboard'), ('racket', 'ball'), ('racket', 'bat'), ('racket', 'glove'), ('bottle', 'cup'), ('bottle', 'fork'), ('bottle', 'knife'), ('bottle', 'spoon'), ('bottle', 'bowl'), ('bottle', 'glass'), ('cup', 'bottle'), ('cup', 'fork'), ('cup', 'knife'), ('cup', 'spoon'), ('cup', 'bowl'), ('cup', 'glass'), ('fork', 'bottle'), ('fork', 'cup'), ('fork', 'knife'), ('fork', 'spoon'), ('fork', 'bowl'), ('fork', 'glass'), ('knife', 'bottle'), ('knife', 'cup'), ('knife', 'fork'), ('knife', 'spoon'), ('knife', 'bowl'), ('knife', 'glass'), ('spoon', 'bottle'), ('spoon', 'cup'), ('spoon', 'fork'), ('spoon', 'knife'), ('spoon', 'bowl'), ('spoon', 'glass'), ('bowl', 'bottle'), ('bowl', 'cup'), ('bowl', 'fork'), ('bowl', 'knife'), ('bowl', 'spoon'), ('bowl', 'glass'), ('glass', 'bottle'), ('glass', 'cup'), ('glass', 'fork'), ('glass', 'knife'), ('glass', 'spoon'), ('glass', 'bowl'), ('banana', 'apple'), ('banana', 'sandwich'), ('banana', 'orange'), ('banana', 'broccoli'), ('banana', 'carrot'), ('banana', 'pizza'), ('banana', 'donut'), ('banana', 'cake'), ('apple', 'banana'), ('apple', 'sandwich'), ('apple', 'orange'), ('apple', 'broccoli'), ('apple', 'carrot'), ('apple', 'pizza'), ('apple', 'donut'), ('apple', 'cake'), ('sandwich', 'banana'), ('sandwich', 'apple'), ('sandwich', 'orange'), ('sandwich', 'broccoli'), ('sandwich', 'carrot'), ('sandwich', 'pizza'), ('sandwich', 'donut'), ('sandwich', 'cake'), ('orange', 'banana'), ('orange', 'apple'), ('orange', 'sandwich'), ('orange', 'broccoli'), ('orange', 'carrot'), ('orange', 'pizza'), ('orange', 'donut'), ('orange', 'cake'), ('broccoli', 'banana'), ('broccoli', 'apple'), ('broccoli', 'sandwich'), ('broccoli', 'orange'), ('broccoli', 'carrot'), ('broccoli', 'pizza'), ('broccoli', 'donut'), ('broccoli', 'cake'), ('carrot', 'banana'), ('carrot', 'apple'), ('carrot', 'sandwich'), ('carrot', 'orange'), ('carrot', 'broccoli'), ('carrot', 'pizza'), ('carrot', 'donut'), ('carrot', 'cake'), ('pizza', 'banana'), ('pizza', 'apple'), ('pizza', 'sandwich'), ('pizza', 'orange'), ('pizza', 'broccoli'), ('pizza', 'carrot'), ('pizza', 'donut'), ('pizza', 'cake'), ('donut', 'banana'), ('donut', 'apple'), ('donut', 'sandwich'), ('donut', 'orange'), ('donut', 'broccoli'), ('donut', 'carrot'), ('donut', 'pizza'), ('donut', 'cake'), ('cake', 'banana'), ('cake', 'apple'), ('cake', 'sandwich'), ('cake', 'orange'), ('cake', 'broccoli'), ('cake', 'carrot'), ('cake', 'pizza'), ('cake', 'donut'), ('chair', 'couch'), ('chair', 'bed'), ('chair', 'toilet'), ('chair', 'plant'), ('chair', 'table'), ('couch', 'chair'), ('couch', 'bed'), ('couch', 'toilet'), ('couch', 'plant'), ('couch', 'table'), ('bed', 'chair'), ('bed', 'couch'), ('bed', 'toilet'), ('bed', 'plant'), ('bed', 'table'), ('toilet', 'chair'), ('toilet', 'couch'), ('toilet', 'bed'), ('toilet', 'plant'), ('toilet', 'table'), ('plant', 'chair'), ('plant', 'couch'), ('plant', 'bed'), ('plant', 'toilet'), ('plant', 'table'), ('table', 'chair'), ('table', 'couch'), ('table', 'bed'), ('table', 'toilet'), ('table', 'plant'), ('tv', 'laptop'), ('tv', 'mouse'), ('tv', 'remote'), ('tv', 'keyboard'), ('tv', 'phone'), ('laptop', 'tv'), ('laptop', 'mouse'), ('laptop', 'remote'), ('laptop', 'keyboard'), ('laptop', 'phone'), ('mouse', 'tv'), ('mouse', 'laptop'), ('mouse', 'remote'), ('mouse', 'keyboard'), ('mouse', 'phone'), ('remote', 'tv'), ('remote', 'laptop'), ('remote', 'mouse'), ('remote', 'keyboard'), ('remote', 'phone'), ('keyboard', 'tv'), ('keyboard', 'laptop'), ('keyboard', 'mouse'), ('keyboard', 'remote'), ('keyboard', 'phone'), ('phone', 'tv'), ('phone', 'laptop'), ('phone', 'mouse'), ('phone', 'remote'), ('phone', 'keyboard'), ('microwave', 'oven'), ('microwave', 'toaster'), ('microwave', 'sink'), ('microwave', 'refrigerator'), ('oven', 'microwave'), ('oven', 'toaster'), ('oven', 'sink'), ('oven', 'refrigerator'), ('toaster', 'microwave'), ('toaster', 'oven'), ('toaster', 'sink'), ('toaster', 'refrigerator'), ('sink', 'microwave'), ('sink', 'oven'), ('sink', 'toaster'), ('sink', 'refrigerator'), ('refrigerator', 'microwave'), ('refrigerator', 'oven'), ('refrigerator', 'toaster'), ('refrigerator', 'sink'), ('book', 'clock'), ('book', 'vase'), ('book', 'scissors'), ('book', 'toothbrush'), ('book', 'drier'), ('clock', 'book'), ('clock', 'vase'), ('clock', 'scissors'), ('clock', 'toothbrush'), ('clock', 'drier'), ('vase', 'book'), ('vase', 'clock'), ('vase', 'scissors'), ('vase', 'toothbrush'), ('vase', 'drier'), ('scissors', 'book'), ('scissors', 'clock'), ('scissors', 'vase'), ('scissors', 'toothbrush'), ('scissors', 'drier'), ('toothbrush', 'book'), ('toothbrush', 'clock'), ('toothbrush', 'vase'), ('toothbrush', 'scissors'), ('toothbrush', 'drier'), ('drier', 'book'), ('drier', 'clock'), ('drier', 'vase'), ('drier', 'scissors'), ('drier', 'toothbrush')]\n"
     ]
    }
   ],
   "source": [
    "target_foil_pairs = []\n",
    "train_pairs = []\n",
    "test_pairs = []\n",
    "\n",
    "for cat in obj_sup_cat:\n",
    "    if len(obj_sup_cat[cat]) > 1: #there are some supercategories with 1 object category\n",
    "        perms = list(itertools.permutations(obj_sup_cat[cat], 2))\n",
    "        target_foil_pairs.extend(perms)\n",
    "        \n",
    "        for p in perms:\n",
    "            if randint(0, 1) == 1:\n",
    "                train_pairs.append(p)\n",
    "            else:\n",
    "                test_pairs.append(p)\n",
    "\n",
    "print(target_foil_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_foil_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_pairs.json', 'w') as file:\n",
    "    json.dump(train_pairs, file)\n",
    "    \n",
    "with open('test_pairs.json', 'w') as file:\n",
    "    json.dump(test_pairs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_sentence(orig_sent, orig_word, foil_word):\n",
    "    #change only the tokens, not accidental ones etc.\n",
    "    \n",
    "    orig_list = orig_sent.split()\n",
    "    \n",
    "    for o in range(len(orig_list)):\n",
    "        \n",
    "        if orig_list[o] == orig_word:\n",
    "            orig_list[o] = foil_word\n",
    "            \n",
    "            \n",
    "    foil_sent = ''\n",
    "    for o in range(len(orig_list)):\n",
    "        foil_sent = foil_sent.join(orig_list)\n",
    "        \n",
    "        if o < len(orig_list)-1:\n",
    "            foil_sent = foil_sent.join(' ')\n",
    "        \n",
    "    return foil_sent\n",
    "\n",
    "\n",
    "def find_object_mentions(captions, obj_cat, lem, constraint_mentions):\n",
    "    \n",
    "    freq_words = defaultdict(int)\n",
    "    \n",
    "    #find the visually salient object words, ones that were mentioned multiple times in the captions\n",
    "    for c in obj_cat:\n",
    "        for cap in captions:\n",
    "            sent = cap[0]\n",
    "            for w in sent.split():\n",
    "                if w.lower() == c or lem.lemmatize(w.lower()) == c: #convert to lowercase and lemmatize            \n",
    "                    freq_words[c] += 1\n",
    "    \n",
    "    #todo: at least 2 user mentioned the word\n",
    "    temp_freq_words = freq_words.copy()\n",
    "    \n",
    "    for f in temp_freq_words:\n",
    "        if freq_words[f] < constraint_mentions:\n",
    "            freq_words.pop(f)\n",
    "            \n",
    "    return freq_words\n",
    "\n",
    "\n",
    "def generate_not_present_foil(captions, img_id, object_instances, salient_words, obj_cat, target_foil_pairs, lem):\n",
    "    \n",
    "    foil_pairs = []\n",
    "    \n",
    "    word_mode = defaultdict(list)\n",
    "    #find potential foil words from the same supercategory as the salient word\n",
    "    for sw in salient_words:\n",
    "            for tf in target_foil_pairs:\n",
    "                if tf[0] == sw:\n",
    "                    potential_foil = tf[1]\n",
    "                    foil_pairs.append((sw, potential_foil))\n",
    "                   \n",
    "    #remove the foil words that were mentioned in the captions\n",
    "    \n",
    "    #NEW! also check the instances set for the object annotations\n",
    "    #remove object categories annotated as being present in the image\n",
    "    \n",
    "    img_id = int(img_id)\n",
    "    obj_insts = object_instances[img_id]\n",
    "    \n",
    "    temp_foil_pairs = foil_pairs\n",
    "    \n",
    "    for fp in temp_foil_pairs:\n",
    "        for c in captions:\n",
    "            sent = c[0]\n",
    "            for w in sent.split():\n",
    "                if w.lower() == fp[1] or lem.lemmatize(w) == fp[1]:  \n",
    "                    if fp in foil_pairs:\n",
    "                        foil_pairs.remove(fp)\n",
    "              \n",
    "        for obj in obj_insts:\n",
    "            if obj == fp[1]: #obj is the same as foil word\n",
    "                if fp in foil_pairs:\n",
    "                    foil_pairs.remove(fp)\n",
    "    \n",
    "    return foil_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse\n",
      "Mice\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "word = 'mice'\n",
    "print(lemmatizer.lemmatize(word))\n",
    "\n",
    "p = inflect.engine()\n",
    "print(p.plural('Mouse'))\n",
    "plurals = defaultdict(str)\n",
    "\n",
    "for obj in obj_cat:\n",
    "    plurals[obj] = p.plural(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "constraint_mentions = 2 #at least 2 caption includes that category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A room with blue walls and a white sink and door .', 49]\n",
      "['Blue and white color scheme in a small bathroom .', 109]\n",
      "['This is a blue and white bathroom with a wall sink and a lifesaver on the wall .', 121]\n",
      "['A blue boat themed bathroom with a life preserver on the wall', 163]\n",
      "['A bathroom with walls that are painted baby blue .', 250]\n",
      "\n",
      "defaultdict(<class 'int'>, {'sink': 2})\n",
      "\n",
      "[('sink', 'microwave'), ('sink', 'oven'), ('sink', 'toaster'), ('sink', 'refrigerator')]\n",
      "['hello', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "for tc in train_captions_per_image['322141']:\n",
    "    print(tc)\n",
    "salient_words = find_object_mentions(train_captions_per_image['322141'], obj_cat, lemmatizer, constraint_mentions)\n",
    "print()\n",
    "print(salient_words)\n",
    "foil_pairs = generate_not_present_foil(train_captions_per_image['203564'], 203564, train_object_instances, salient_words, obj_cat, target_foil_pairs, lemmatizer)\n",
    "print()\n",
    "print(foil_pairs)\n",
    "\n",
    "with MosesTokenizer('en') as tokenize:\n",
    "    sent = 'hello world!'\n",
    "    print(tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokenized_captions(captions):\n",
    "    \n",
    "    tokenized_captions = []\n",
    "    \n",
    "    for caption in captions:\n",
    "        with MosesTokenizer('en') as tokenize:\n",
    "                    \n",
    "            temp_cap = caption[0].replace('\\n', '')\n",
    "            sentence = tokenize(temp_cap)\n",
    "            \n",
    "        #now punctuation marks are all separated\n",
    "        #combine the tokens with a space between them\n",
    "        new_sentence = ''\n",
    "        for t in range(len(sentence)):\n",
    "            new_sentence += sentence[t]\n",
    "            \n",
    "            if t < (len(sentence)-1):\n",
    "                new_sentence += ' '\n",
    "                \n",
    "        temp_caption = (new_sentence, caption[1])#tokenized_caption, sentence_id\n",
    "        \n",
    "        tokenized_captions.append(temp_caption)\n",
    "    \n",
    "    return tokenized_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokenized_sentence(sentence):\n",
    "    \n",
    "    tokenized_sentence = []\n",
    "    \n",
    "    with MosesTokenizer('en') as tokenize:\n",
    "            \n",
    "        sentence = sentence.replace('\\n', '')\n",
    "        sentence = tokenize(sentence)\n",
    "        #now punctuation marks are all separated\n",
    "        #combine the tokens with a space between them\n",
    "        new_sentence = ''\n",
    "        for t in range(len(sentence)):\n",
    "            new_sentence += sentence[t]\n",
    "            \n",
    "            if t < (len(sentence)-1):\n",
    "                new_sentence += ' '\n",
    "                \n",
    "       \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118287"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_captions_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foil_sentences= defaultdict(list)\n",
    "\n",
    "count = 0\n",
    "#for each image\n",
    "for tp in train_captions_per_image:\n",
    "    #all 5 captions of an image\n",
    "    \n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    \n",
    "    count +=1\n",
    "    \n",
    "    captions = train_captions_per_image[tp]\n",
    "    \n",
    "    #find the visually salient words given all captions for an image\n",
    "    vis_sal_words = find_object_mentions(captions, obj_cat, lemmatizer,constraint_mentions)\n",
    "    \n",
    "    #find possible foil words for the captions\n",
    "    \n",
    "    #IMPORTANT CHANGE: target_foil_pairs to train_pairs\n",
    "    #also removing visually present objects\n",
    "    foil_pairs =  generate_not_present_foil(captions, tp, train_object_instances, vis_sal_words, obj_cat, test_pairs, lemmatizer)\n",
    "     \n",
    "    for caption in captions:\n",
    "        \n",
    "        sent = caption[0]\n",
    "        sentid = caption[1]\n",
    "        \n",
    "        for w in sent.split():\n",
    "\n",
    "            #if the word is a salient word\n",
    "            if w in vis_sal_words:\n",
    "                \n",
    "                for fp in foil_pairs:\n",
    "                    \n",
    "                    #check if that word is an original word in the list of target-foil pairs\n",
    "                    #convert the sentence into foil sentence \n",
    "                    #check for singular-plural form and upper,lower,title cases\n",
    "                    \n",
    "                    if w == fp[0]:\n",
    "                        foil_sentences[int(tp)].append([convert_sentence(sent, w, fp[1]), w , fp[1], sentid])\n",
    "                        \n",
    "                    elif w == fp[0].upper():\n",
    "                        foil_sentences[int(tp)].append([convert_sentence(sent, w, fp[1].upper()), w , fp[1].upper(), sentid])\n",
    "                        \n",
    "                    elif w == fp[0].title():    \n",
    "                        foil_sentences[int(tp)].append([convert_sentence(sent, w, fp[1].title()), w , fp[1].title(), sentid])\n",
    "                        \n",
    "                    elif p.singular_noun(w) == fp[0]:\n",
    "                        foil_sentences[int(tp)].append([convert_sentence(sent, w, p.plural(fp[1])), w , p.plural(fp[1]), sentid])\n",
    "                        \n",
    "                    elif p.singular_noun(w) == fp[0].upper():\n",
    "                        foil_sentences[int(tp)].append([convert_sentence(sent, w, p.plural(fp[1]).upper()), w , p.plural(fp[1]).upper(), sentid])\n",
    "                        \n",
    "                    elif p.singular_noun(w) == fp[0].title():\n",
    "                        foil_sentences[int(tp)].append([convert_sentence(sent, w, p.plural(fp[1]).title()), w , p.plural(fp[1]).title(), sentid])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foil_sentences[203564] #foil sentence, orig word, foil word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_foil_sentences_ece.json', 'w') as f:\n",
    "    json.dump(foil_sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val_captions_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foil_sentences_val= defaultdict(list)\n",
    "\n",
    "#for each image\n",
    "count = 0\n",
    "for tp in val_captions_per_image:\n",
    "    \n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    \n",
    "    count +=1\n",
    "    #all 5 captions of an image\n",
    "    captions = val_captions_per_image[tp]\n",
    "    \n",
    "    #find the visually salient words given all captions for an image\n",
    "    vis_sal_words = find_object_mentions(captions, obj_cat, lemmatizer,constraint_mentions)\n",
    "    \n",
    "    #find possible foil words for the captions\n",
    "    \n",
    "    #IMPORTANT CHANGE: target_foil_pairs to test_pairs\n",
    "    foil_pairs =  generate_not_present_foil(captions, tp, val_object_instances, vis_sal_words, obj_cat, test_pairs, lemmatizer)\n",
    "     \n",
    "    for caption in captions:\n",
    "        \n",
    "        sent = caption[0]\n",
    "        sentid = caption[1]\n",
    "        \n",
    "        for w in sent.split():\n",
    "            \n",
    "             #if the word is a salient word\n",
    "            if w in vis_sal_words:\n",
    "                \n",
    "                for fp in foil_pairs:\n",
    "                    \n",
    "                    #check if that word is an original word in the list of target-foil pairs\n",
    "                    #convert the sentence into foil sentence \n",
    "                    #check for singular-plural form and upper,lower,title cases\n",
    "                    \n",
    "                    if w == fp[0]:\n",
    "                        foil_sentences_val[int(tp)].append([convert_sentence(sent, w, fp[1]), w , fp[1], sentid])\n",
    "                        \n",
    "                    elif w == fp[0].upper():\n",
    "                        foil_sentences_val[int(tp)].append([convert_sentence(sent, w, fp[1].upper()), w , fp[1].upper(), sentid])\n",
    "                        \n",
    "                    elif w == fp[0].title():    \n",
    "                        foil_sentences_val[int(tp)].append([convert_sentence(sent, w, fp[1].title()), w , fp[1].title(), sentid])\n",
    "                        \n",
    "                    elif p.singular_noun(w) == fp[0]:\n",
    "                        foil_sentences_val[int(tp)].append([convert_sentence(sent, w, p.plural(fp[1])), w , p.plural(fp[1]), sentid])\n",
    "                        \n",
    "                    elif p.singular_noun(w) == fp[0].upper():\n",
    "                        foil_sentences_val[int(tp)].append([convert_sentence(sent, w, p.plural(fp[1]).upper()), w , p.plural(fp[1]).upper(), sentid])\n",
    "                        \n",
    "                    elif p.singular_noun(w) == fp[0].title():\n",
    "                        foil_sentences_val[int(tp)].append([convert_sentence(sent, w, p.plural(fp[1]).title()), w , p.plural(fp[1]).title(), sentid])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A black Honda bicycle parked in front of a garage .',\n",
       "  'motorcycle',\n",
       "  'bicycle',\n",
       "  38],\n",
       " ['A black Honda car parked in front of a garage .', 'motorcycle', 'car', 38],\n",
       " ['A black Honda bus parked in front of a garage .', 'motorcycle', 'bus', 38],\n",
       " ['A black Honda train parked in front of a garage .',\n",
       "  'motorcycle',\n",
       "  'train',\n",
       "  38],\n",
       " ['A black Honda truck parked in front of a garage .',\n",
       "  'motorcycle',\n",
       "  'truck',\n",
       "  38],\n",
       " ['A Honda bicycle parked in a grass driveway', 'motorcycle', 'bicycle', 182],\n",
       " ['A Honda car parked in a grass driveway', 'motorcycle', 'car', 182],\n",
       " ['A Honda bus parked in a grass driveway', 'motorcycle', 'bus', 182],\n",
       " ['A Honda train parked in a grass driveway', 'motorcycle', 'train', 182],\n",
       " ['A Honda truck parked in a grass driveway', 'motorcycle', 'truck', 182],\n",
       " ['A black Honda bicycle with a dark burgundy seat .',\n",
       "  'motorcycle',\n",
       "  'bicycle',\n",
       "  479],\n",
       " ['A black Honda car with a dark burgundy seat .', 'motorcycle', 'car', 479],\n",
       " ['A black Honda bus with a dark burgundy seat .', 'motorcycle', 'bus', 479],\n",
       " ['A black Honda train with a dark burgundy seat .',\n",
       "  'motorcycle',\n",
       "  'train',\n",
       "  479],\n",
       " ['A black Honda truck with a dark burgundy seat .',\n",
       "  'motorcycle',\n",
       "  'truck',\n",
       "  479],\n",
       " ['Ma bicycle parked on the gravel in front of a garage',\n",
       "  'motorcycle',\n",
       "  'bicycle',\n",
       "  6638],\n",
       " ['Ma car parked on the gravel in front of a garage',\n",
       "  'motorcycle',\n",
       "  'car',\n",
       "  6638],\n",
       " ['Ma bus parked on the gravel in front of a garage',\n",
       "  'motorcycle',\n",
       "  'bus',\n",
       "  6638],\n",
       " ['Ma train parked on the gravel in front of a garage',\n",
       "  'motorcycle',\n",
       "  'train',\n",
       "  6638],\n",
       " ['Ma truck parked on the gravel in front of a garage',\n",
       "  'motorcycle',\n",
       "  'truck',\n",
       "  6638],\n",
       " ['A bicycle with its brake extended standing outside',\n",
       "  'motorcycle',\n",
       "  'bicycle',\n",
       "  6803],\n",
       " ['A car with its brake extended standing outside', 'motorcycle', 'car', 6803],\n",
       " ['A bus with its brake extended standing outside', 'motorcycle', 'bus', 6803],\n",
       " ['A train with its brake extended standing outside',\n",
       "  'motorcycle',\n",
       "  'train',\n",
       "  6803],\n",
       " ['A truck with its brake extended standing outside',\n",
       "  'motorcycle',\n",
       "  'truck',\n",
       "  6803]]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foil_sentences_val[179765]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A black Honda motorcycle parked in front of a garage .', 38],\n",
       " ['A Honda motorcycle parked in a grass driveway', 182],\n",
       " ['A black Honda motorcycle with a dark burgundy seat .', 479],\n",
       " ['Ma motorcycle parked on the gravel in front of a garage', 6638],\n",
       " ['A motorcycle with its brake extended standing outside', 6803]]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions_per_image['179765']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('val_foil_sentences_ece.json', 'w') as f:\n",
    "    json.dump(foil_sentences_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coco_lookup(coco):\n",
    "    \n",
    "    lookup_table = defaultdict(dict)\n",
    "    \n",
    "    for im in coco['images']:\n",
    "        \n",
    "        cocoid = im['cocoid']\n",
    "        \n",
    "        filename = im['filename']\n",
    "        filepath = im['filepath']\n",
    "        \n",
    "        imgid = im['imgid']\n",
    "        \n",
    "        sentids = im['sentids']\n",
    "        \n",
    "        sentences = im['sentences']\n",
    "        \n",
    "        lookup_table[cocoid] = {'filename' : filename, 'filepath':filepath, 'imgid': imgid, 'sentids':sentids, 'sentences':sentences}\n",
    "    \n",
    "    return lookup_table\n",
    "\n",
    "\n",
    "with open(\"neuraltalk-master/data/coco/coco_dataset.json\") as in_file:\n",
    "    coco = json.load(in_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO_val2014_000000203564.jpg\n",
      "dict_keys(['images', 'dataset'])\n",
      "{'filepath': 'train2014', 'sentids': [474367, 474697, 475306, 480184, 819358], 'filename': 'COCO_train2014_000000147610.jpg', 'imgid': 100000, 'split': 'train', 'sentences': [{'tokens': ['a', 'motorcycle', 'is', 'placed', 'outside', 'of', 'the', 'house'], 'raw': 'A motorcycle is placed outside of the house. ', 'imgid': 100000, 'sentid': 474367}, {'tokens': ['a', 'motorcycle', 'parked', 'in', 'front', 'of', 'a', 'red', 'brick', 'wall'], 'raw': 'A motorcycle parked in front of a red brick wall.', 'imgid': 100000, 'sentid': 474697}, {'tokens': ['a', 'motorcycle', 'is', 'parked', 'on', 'a', 'dirt', 'road'], 'raw': 'A motorcycle is parked on a dirt road.', 'imgid': 100000, 'sentid': 475306}, {'tokens': ['a', 'motorbike', 'is', 'parked', 'at', 'the', 'bottom', 'of', 'the', 'hillside', 'near', 'the', 'pagoda', 'style', 'homes'], 'raw': 'A motorbike is parked at the bottom of the hillside near the pagoda-style homes.', 'imgid': 100000, 'sentid': 480184}, {'tokens': ['a', 'parked', 'motorcycle', 'on', 'a', 'dirt', 'road', 'in', 'front', 'of', 'an', 'old', 'building'], 'raw': 'A parked motorcycle on a dirt road in front of an old building.', 'imgid': 100000, 'sentid': 819358}], 'cocoid': 147610}\n"
     ]
    }
   ],
   "source": [
    "lookup = coco_lookup(coco)\n",
    "print(lookup[203564]['filename'])\n",
    "print(coco.keys())\n",
    "print(coco['images'][100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_foil_sentences_ece.json', 'r') as f:\n",
    "    foil_sentences = json.load(f)\n",
    "    \n",
    "with open('val_foil_sentences_ece.json', 'r') as f:\n",
    "    foil_sentences_val = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75466"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(foil_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_check_dataset = defaultdict(list)\n",
    "train_check_dataset['dataset'] = 'train_check'\n",
    "\n",
    "#create the dataset so that it can be utilized in NeuralTalk\n",
    "\n",
    "for ft in foil_sentences:\n",
    "    \n",
    "    sentences = foil_sentences[ft]\n",
    "    \n",
    "    cocoid = int(ft)\n",
    "    \n",
    "    split = 'train'\n",
    "\n",
    "    filename = lookup[cocoid]['filename']\n",
    "    filepath = lookup[cocoid]['filepath']\n",
    "    imgid = lookup[cocoid]['imgid']\n",
    "    posTag = 'Original' \n",
    "\n",
    "    #first the original captions for this cocoid\n",
    "    \n",
    "    for org_cap in train_captions_per_image[str(cocoid)]:\n",
    "        \n",
    "        sent = org_cap[0]\n",
    "        \n",
    "        sentid = org_cap[1]\n",
    "        \n",
    "        #print(sent)\n",
    "        changeid = 'Original'\n",
    "\n",
    "        sentids = [sentid]\n",
    "\n",
    "        targetfoilPair = 'Original'\n",
    "        \n",
    "        im_dict = defaultdict()\n",
    "        \n",
    "        im_dict['change_id'] = changeid\n",
    "        im_dict['cocoid'] = cocoid\n",
    "        im_dict['filename'] = filename\n",
    "        im_dict['filepath'] = filepath\n",
    "        im_dict['imgid'] = imgid\n",
    "        im_dict['posTag'] = posTag\n",
    "        im_dict['sentids'] = sentids\n",
    "        im_dict['split'] = split\n",
    "        im_dict['targetfoilPair'] = targetfoilPair\n",
    "\n",
    "\n",
    "        im_sent_dict = defaultdict()\n",
    "        \n",
    "        im_sent_dict['imgid'] = imgid\n",
    "        im_sent_dict['raw'] = sent\n",
    "        im_sent_dict['sentid'] = sentid\n",
    "        im_sent_dict['tokens'] = sent.split()\n",
    "\n",
    "        im_dict['sentences'] = [im_sent_dict]\n",
    "      \n",
    "        train_check_dataset['images'].append(im_dict)\n",
    "\n",
    "    c = 1 #foil counter per caption\n",
    "    \n",
    "    #then all the foil captions\n",
    "    for s in sentences:\n",
    "    \n",
    "        #print(s)\n",
    "        foil_sent = s[0]\n",
    "        pair0 = s[1]\n",
    "        pair1 = s[2]\n",
    "        sentid = s[3]\n",
    "\n",
    "        targetfoilPair = pair0 + '_' + pair1\n",
    "        \n",
    "        changeid = str(sentid) + '_' + str(c)\n",
    "        c += 1\n",
    "        sentids = [sentid]\n",
    "        \n",
    "        posTag = 'noun'\n",
    "        \n",
    "        im_dict = defaultdict()\n",
    "        \n",
    "        im_dict['change_id'] = changeid\n",
    "        im_dict['cocoid'] = cocoid\n",
    "        im_dict['filename'] = filename\n",
    "        im_dict['filepath'] = filepath\n",
    "        im_dict['imgid'] = imgid\n",
    "        im_dict['posTag'] = posTag\n",
    "        im_dict['sentids'] = sentids\n",
    "        im_dict['split'] = split\n",
    "        im_dict['targetfoilPair'] = targetfoilPair\n",
    "\n",
    "\n",
    "        im_sent_dict = defaultdict()\n",
    "        \n",
    "        im_sent_dict['imgid'] = imgid\n",
    "        im_sent_dict['raw'] = foil_sent\n",
    "        im_sent_dict['sentid'] = sentid\n",
    "        im_sent_dict['tokens'] = foil_sent.split()\n",
    "\n",
    "        im_dict['sentences'] = [im_sent_dict]\n",
    "      \n",
    "        train_check_dataset['images'].append(im_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(None,\n",
       "             {'imgid': 30770,\n",
       "              'raw': 'A bus replica with a clock as the front wheel .',\n",
       "              'sentid': 37,\n",
       "              'tokens': ['A',\n",
       "               'bus',\n",
       "               'replica',\n",
       "               'with',\n",
       "               'a',\n",
       "               'clock',\n",
       "               'as',\n",
       "               'the',\n",
       "               'front',\n",
       "               'wheel',\n",
       "               '.']})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_check_dataset['images'][5]['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_dataset.json', 'w') as f:\n",
    "    json.dump(train_check_dataset, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_check_dataset = defaultdict(list)\n",
    "test_check_dataset['dataset'] = 'test_check'\n",
    "\n",
    "for ft in foil_sentences_val:\n",
    "    \n",
    "    sentences = foil_sentences_val[ft]\n",
    "    \n",
    "    cocoid = int(ft)\n",
    "    \n",
    "    split = 'test'\n",
    "\n",
    "    filename = lookup[cocoid]['filename']\n",
    "    filepath = lookup[cocoid]['filepath']\n",
    "    imgid = lookup[cocoid]['imgid']\n",
    "    posTag = 'Original' \n",
    "\n",
    "    #first the original captions for this cocoid\n",
    "    \n",
    "    for org_cap in val_captions_per_image[str(cocoid)]:\n",
    "        \n",
    "        sent = org_cap[0]\n",
    "        print(sent)\n",
    "        sentid = org_cap[1]\n",
    "        \n",
    "        #print(sent)\n",
    "        changeid = 'Original'\n",
    "\n",
    "        sentids = [sentid]\n",
    "\n",
    "        im_dict = defaultdict()\n",
    "        \n",
    "        targetfoilPair = 'Original'\n",
    "        \n",
    "        im_dict['change_id'] = changeid\n",
    "        im_dict['cocoid'] = cocoid\n",
    "        im_dict['filename'] = filename\n",
    "        im_dict['filepath'] = filepath\n",
    "        im_dict['imgid'] = imgid\n",
    "        im_dict['posTag'] = posTag\n",
    "        im_dict['sentids'] = sentids\n",
    "        im_dict['split'] = split\n",
    "        im_dict['targetfoilPair'] = targetfoilPair\n",
    "\n",
    "\n",
    "        im_sent_dict = defaultdict()\n",
    "        \n",
    "        im_sent_dict['imgid'] = imgid\n",
    "        im_sent_dict['raw'] = sent\n",
    "        im_sent_dict['sentid'] = sentid\n",
    "        im_sent_dict['tokens'] = sent.split()\n",
    "\n",
    "        im_dict['sentences'] = [im_sent_dict]\n",
    "      \n",
    "        test_check_dataset['images'].append(im_dict)\n",
    "        \n",
    "\n",
    "    c = 1 #foil counter per caption\n",
    "    \n",
    "    #then all the foil captions\n",
    "    for s in sentences:\n",
    "    \n",
    "        #print(s)\n",
    "        foil_sent = s[0]\n",
    "        pair0 = s[1]\n",
    "        pair1 = s[2]\n",
    "        sentid = s[3]\n",
    "\n",
    "        targetfoilPair = pair0 + '_' + pair1\n",
    "        \n",
    "        changeid = str(sentid) + '_' + str(c)\n",
    "        c += 1\n",
    "        sentids = [sentid]\n",
    "        \n",
    "        posTag = 'noun'\n",
    "        \n",
    "        im_dict = defaultdict()\n",
    "        \n",
    "        im_dict['change_id'] = changeid\n",
    "        im_dict['cocoid'] = cocoid\n",
    "        im_dict['filename'] = filename\n",
    "        im_dict['filepath'] = filepath\n",
    "        im_dict['imgid'] = imgid\n",
    "        im_dict['posTag'] = posTag\n",
    "        im_dict['sentids'] = sentids\n",
    "        im_dict['split'] = split\n",
    "        im_dict['targetfoilPair'] = targetfoilPair\n",
    "\n",
    "\n",
    "        im_sent_dict = defaultdict()\n",
    "        \n",
    "        im_sent_dict['imgid'] = imgid\n",
    "        im_sent_dict['raw'] = foil_sent\n",
    "        im_sent_dict['sentid'] = sentid\n",
    "        im_sent_dict['tokens'] = foil_sent.split()\n",
    "\n",
    "        im_dict['sentences'] = [im_sent_dict]\n",
    "      \n",
    "        test_check_dataset['images'].append(im_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'change_id': 'Original',\n",
       "             'cocoid': 179765,\n",
       "             'filename': 'COCO_val2014_000000179765.jpg',\n",
       "             'filepath': 'val2014',\n",
       "             'imgid': 26743,\n",
       "             'posTag': 'Original',\n",
       "             'sentences': [defaultdict(None,\n",
       "                          {'imgid': 26743,\n",
       "                           'raw': 'A Honda motorcycle parked in a grass driveway',\n",
       "                           'sentid': 182,\n",
       "                           'tokens': ['A',\n",
       "                            'Honda',\n",
       "                            'motorcycle',\n",
       "                            'parked',\n",
       "                            'in',\n",
       "                            'a',\n",
       "                            'grass',\n",
       "                            'driveway']})],\n",
       "             'sentids': [182],\n",
       "             'split': 'test',\n",
       "             'targetfoilPair': 'Original'})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_check_dataset['images'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test_dataset.json', 'w') as f:\n",
    "    json.dump(test_check_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"annotations_trainval/instances_train2017.json\") as in_file:\n",
    "    instances_train = json.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"annotations_trainval/instances_val2017.json\") as in_file:\n",
    "    instances_val = json.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'licenses', 'images', 'annotations', 'categories'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': 2765.1486500000005,\n",
       " 'bbox': [199.84, 200.46, 77.71, 70.88],\n",
       " 'category_id': 58,\n",
       " 'id': 156,\n",
       " 'image_id': 558840,\n",
       " 'iscrowd': 0,\n",
       " 'segmentation': [[239.97,\n",
       "   260.24,\n",
       "   222.04,\n",
       "   270.49,\n",
       "   199.84,\n",
       "   253.41,\n",
       "   213.5,\n",
       "   227.79,\n",
       "   259.62,\n",
       "   200.46,\n",
       "   274.13,\n",
       "   202.17,\n",
       "   277.55,\n",
       "   210.71,\n",
       "   249.37,\n",
       "   253.41,\n",
       "   237.41,\n",
       "   264.51,\n",
       "   242.54,\n",
       "   261.95,\n",
       "   228.87,\n",
       "   271.34]]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances_train['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "44\n",
      "44\n",
      "47\n",
      "1\n",
      "44\n",
      "50\n",
      "1\n",
      "44\n",
      "67\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "for inst in instances_train['annotations']:\n",
    "    if inst['image_id'] == 558840:\n",
    "        print(inst['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furniture dining table\n"
     ]
    }
   ],
   "source": [
    "for cat in instances_train['categories']:\n",
    "    if cat['id']== 67:\n",
    "        print(cat['supercategory'],cat['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_category_lookup(instances_set):\n",
    "    \n",
    "    category_lookup = defaultdict(lambda: defaultdict(str))\n",
    "    \n",
    "    for cat in instances_train['categories']:\n",
    "        \n",
    "        category_lookup[cat['id']] = {'name':cat['name'], 'supercategory':cat['supercategory']}\n",
    "    \n",
    "    return category_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_lookup = get_category_lookup(instances_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_object_instances(instances_set):\n",
    "    \n",
    "    instances = defaultdict(set)\n",
    "    \n",
    "    for inst in instances_set['annotations']:\n",
    "        category_name_supcat = category_lookup[inst['category_id']]\n",
    "        instances[inst['image_id']].add(category_name_supcat['name'])\n",
    "        \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_object_instances = get_image_object_instances(instances_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_object_instances = get_image_object_instances(instances_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bottle', 'cup', 'dining table', 'hot dog', 'person', 'spoon'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_object_instances[558840]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_multiword_obj(categories):\n",
    "    \n",
    "#     multiword_dicts = defaultdict(list)\n",
    "    \n",
    "#     for cat in categories:\n",
    "#         name = cat['name']\n",
    "        \n",
    "#         words = name.split()\n",
    "        \n",
    "#         if len(words) > 1:\n",
    "#             #multiword\n",
    "            \n",
    "#             first = words[0]\n",
    "#             second = words[1]\n",
    "#             concat_without_space = words[0] + words[1]\n",
    "    \n",
    "        \n",
    "#             multiword_dicts[name] = [first, second, concat_without_space]\n",
    "            \n",
    "    \n",
    "#     return multiword_dicts\n",
    "    \n",
    "\n",
    "# multiword_dicts = get_multiword_obj(test_cap['categories'])\n",
    "\n",
    "# multiword_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
